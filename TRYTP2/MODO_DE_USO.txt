================================================================================
                    TP2 - SISTEMA DE SCRAPING WEB DISTRIBUIDO
                            MODO DE USO COMPLETO
================================================================================

üìã √çNDICE
---------
1. Instalaci√≥n R√°pida
2. Iniciar los Servidores
3. Usar el Cliente
4. Ejemplos Pr√°cticos
5. Opciones Avanzadas
6. Soluci√≥n de Problemas
7. Comandos √ötiles


================================================================================
1. INSTALACI√ìN R√ÅPIDA
================================================================================

Paso 1: Crear entorno virtual
------------------------------
# Linux/Mac
python3 -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate


Paso 2: Instalar dependencias
-----------------------------
pip install -r requirements.txt


Paso 3: Instalar ChromeDriver (OPCIONAL - para screenshots)
-----------------------------------------------------------
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install chromium-chromedriver

# macOS
brew install chromedriver

# Windows
Descargar de: https://chromedriver.chromium.org/

# Alternativa m√°s f√°cil - Playwright
pip install playwright
playwright install chromium


================================================================================
2. INICIAR LOS SERVIDORES
================================================================================

IMPORTANTE: Debes iniciar PRIMERO el servidor de procesamiento (Parte B)
         y LUEGO el servidor de scraping (Parte A)


Terminal 1: Servidor de Procesamiento (Parte B)
-----------------------------------------------
python3 server_processing.py -i 127.0.0.1 -p 9000

Ver√°s:
  Starting Processing Server on 127.0.0.1:9000
  Process pool size: 8
  Server ready to accept connections


Terminal 2: Servidor de Scraping (Parte A)
------------------------------------------
python3 server_scraping.py -i 127.0.0.1 -p 8000

Ver√°s:
  Starting Scraping Server on 127.0.0.1:8000
  Workers: 4
  Processing Server: 127.0.0.1:9000


¬°Listo! El sistema est√° corriendo üöÄ


================================================================================
3. USAR EL CLIENTE
================================================================================

Terminal 3: Cliente de Prueba
-----------------------------

Scraping b√°sico:
----------------
python3 client.py --url https://example.com


Ver solo salud del servidor:
---------------------------
python3 client.py --health


Usar m√©todo POST:
-----------------
python3 client.py --url https://python.org --post


Obtener resultado en JSON:
--------------------------
python3 client.py --url https://github.com --json


Servidor remoto:
---------------
python3 client.py --url https://example.com --host 192.168.1.10 --port 8000


================================================================================
4. EJEMPLOS PR√ÅCTICOS
================================================================================

Ejemplo 1: Scrapear una p√°gina simple
-------------------------------------
python3 client.py --url https://example.com

Resultado:
  üìä RESULTS
  ‚úÖ Status: success
  üìå Title: Example Domain
  üîó Links Found: 1
  üñºÔ∏è  Images Count: 0


Ejemplo 2: Scrapear sitio complejo
----------------------------------
python3 client.py --url https://python.org

Resultado mostrar√°:
  - T√≠tulo de la p√°gina
  - Todos los enlaces encontrados
  - Meta tags (description, keywords, Open Graph)
  - Estructura HTML (H1, H2, H3, etc.)
  - Screenshots (si Selenium est√° instalado)
  - M√©tricas de rendimiento
  - Thumbnails de im√°genes


Ejemplo 3: Usando curl (m√©todo GET)
-----------------------------------
curl "http://localhost:8000/scrape?url=https://example.com" | jq


Ejemplo 4: Usando curl (m√©todo POST)
------------------------------------
curl -X POST http://localhost:8000/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://python.org"}' | jq


Ejemplo 5: Scrapear m√∫ltiples URLs en lote
------------------------------------------
python3 examples/batch_scraper.py \
  --urls https://python.org https://github.com https://stackoverflow.com \
  --concurrent 3 \
  --output results.json


Ejemplo 6: Monitorear estado del servidor
-----------------------------------------
python3 examples/monitor.py --interval 5


================================================================================
5. OPCIONES AVANZADAS
================================================================================

Servidor de Scraping (server_scraping.py)
-----------------------------------------
Uso: python3 server_scraping.py -i IP -p PORT [opciones]

Opciones:
  -i, --ip IP                  Direcci√≥n de escucha (IPv4 o IPv6)
  -p, --port PORT              Puerto de escucha
  -w, --workers N              N√∫mero de workers async (default: 4)
  --processing-host HOST       IP del servidor de procesamiento
  --processing-port PORT       Puerto del servidor de procesamiento

Ejemplos:
  # IPv4 b√°sico
  python3 server_scraping.py -i 127.0.0.1 -p 8000
  
  # IPv6
  python3 server_scraping.py -i ::1 -p 8000
  
  # M√°s workers (mayor concurrencia)
  python3 server_scraping.py -i 0.0.0.0 -p 8000 -w 10
  
  # Servidor de procesamiento remoto
  python3 server_scraping.py -i 0.0.0.0 -p 8000 \
    --processing-host 192.168.1.100 --processing-port 9000


Servidor de Procesamiento (server_processing.py)
------------------------------------------------
Uso: python3 server_processing.py -i IP -p PORT [opciones]

Opciones:
  -i, --ip IP                  Direcci√≥n de escucha
  -p, --port PORT              Puerto de escucha
  -n, --processes N            N√∫mero de procesos (default: CPU cores)

Ejemplos:
  # B√°sico
  python3 server_processing.py -i 127.0.0.1 -p 9000
  
  # M√°s procesos (CPU-bound tasks)
  python3 server_processing.py -i 127.0.0.1 -p 9000 -n 16
  
  # Escuchar en todas las interfaces
  python3 server_processing.py -i 0.0.0.0 -p 9000


Cliente (client.py)
------------------
Uso: python3 client.py [opciones]

Opciones:
  --host HOST                  Host del servidor (default: 127.0.0.1)
  --port PORT                  Puerto del servidor (default: 8000)
  --url URL                    URL a scrapear
  --post                       Usar m√©todo POST
  --health                     Health check
  --json                       Salida en formato JSON

Ejemplos:
  # B√°sico
  python3 client.py --url https://example.com
  
  # POST + JSON
  python3 client.py --url https://python.org --post --json
  
  # Health check
  python3 client.py --health


================================================================================
6. SOLUCI√ìN DE PROBLEMAS
================================================================================

Problema: "Connection refused" al iniciar servidor de scraping
--------------------------------------------------------------
Causa: El servidor de procesamiento no est√° corriendo
Soluci√≥n: Iniciar primero el servidor de procesamiento

  Terminal 1: python3 server_processing.py -i 127.0.0.1 -p 9000
  Terminal 2: python3 server_scraping.py -i 127.0.0.1 -p 8000


Problema: "Address already in use"
----------------------------------
Causa: El puerto ya est√° siendo usado
Soluci√≥n: 
  1. Cambiar de puerto:
     python3 server_scraping.py -i 127.0.0.1 -p 8080
  
  2. O matar el proceso que lo usa:
     # Linux/Mac
     lsof -ti:8000 | xargs kill -9
     
     # Windows
     netstat -ano | findstr :8000
     taskkill /PID <PID> /F


Problema: Screenshots no funcionan
----------------------------------
Causa: ChromeDriver no instalado o Selenium no disponible
Soluci√≥n:
  1. Instalar ChromeDriver (ver secci√≥n 1)
  2. O usar sin screenshots (el resto funciona perfectamente)
  3. Verificar: python3 -c "from selenium import webdriver; print('OK')"


Problema: "Module not found"
----------------------------
Causa: Dependencias no instaladas
Soluci√≥n:
  pip install -r requirements.txt


Problema: Timeout al scrapear p√°ginas
-------------------------------------
Causa: P√°gina muy lenta o inaccesible
Soluci√≥n: El sistema tiene timeout de 30s, esto es normal para p√°ginas lentas


Problema: Error de encoding
---------------------------
Causa: Caracteres especiales en p√°ginas
Soluci√≥n: Ya est√° manejado con errors='ignore' en el c√≥digo


================================================================================
7. COMANDOS √öTILES
================================================================================

Ver logs en tiempo real:
-----------------------
# Si usaste los scripts de inicio
tail -f logs/scraping.log
tail -f logs/processing.log


Detener todos los servidores:
-----------------------------
# Linux/Mac
pkill -f server_scraping.py
pkill -f server_processing.py

# Windows
taskkill /F /IM python.exe


Ver procesos corriendo:
----------------------
# Linux/Mac
ps aux | grep server_

# Windows
tasklist | findstr python


Verificar puertos en uso:
------------------------
# Linux/Mac
lsof -i :8000
lsof -i :9000

# Windows
netstat -ano | findstr :8000
netstat -ano | findstr :9000


Ejecutar tests:
--------------
# Todos los tests
pytest tests/ -v

# Solo tests de scraper
pytest tests/test_scraper.py -v

# Con coverage
pytest tests/ --cov=. --cov-report=html


Verificar instalaci√≥n:
---------------------
python3 -c "import aiohttp, bs4, lxml; print('‚úÖ Dependencias OK')"


Actualizar dependencias:
-----------------------
pip install --upgrade -r requirements.txt


================================================================================
FLUJO COMPLETO DE USO
================================================================================

1. Activar entorno virtual
   source venv/bin/activate

2. Iniciar servidor de procesamiento
   python3 server_processing.py -i 127.0.0.1 -p 9000

3. Iniciar servidor de scraping (nueva terminal)
   python3 server_scraping.py -i 127.0.0.1 -p 8000

4. Probar con cliente (nueva terminal)
   python3 client.py --url https://example.com

5. Ver resultados en pantalla

6. Para detener: Ctrl+C en cada terminal


================================================================================
FORMATO DE RESPUESTA JSON
================================================================================

El servidor devuelve JSON con esta estructura:

{
  "url": "https://example.com",
  "timestamp": "2024-11-10T15:30:00Z",
  "status": "success",
  "scraping_data": {
    "title": "Example Domain",
    "links": ["https://...", "..."],
    "meta_tags": {
      "description": "...",
      "keywords": "...",
      "og_title": "..."
    },
    "images_count": 15,
    "structure": {
      "h1": 2,
      "h2": 5,
      "h3": 10
    }
  },
  "processing_data": {
    "screenshot": "base64_encoded_image...",
    "performance": {
      "load_time_ms": 1250,
      "total_size_kb": 2048,
      "num_requests": 45
    },
    "thumbnails": ["base64_thumb1", "base64_thumb2"]
  }
}


================================================================================
ENDPOINTS DISPONIBLES
================================================================================

GET /scrape?url=<URL>
--------------------
Scrapear una URL espec√≠fica

Ejemplo:
  curl "http://localhost:8000/scrape?url=https://example.com"


POST /scrape
-----------
Scrapear una URL (m√©todo POST)

Ejemplo:
  curl -X POST http://localhost:8000/scrape \
    -H "Content-Type: application/json" \
    -d '{"url": "https://example.com"}'


GET /health
----------
Verificar estado del servidor

Ejemplo:
  curl http://localhost:8000/health


================================================================================
ARQUITECTURA DEL SISTEMA
================================================================================

Cliente HTTP
    ‚Üì
Servidor A (Asyncio) ‚Üê‚Üí Servidor B (Multiprocessing)
    ‚îÇ                         ‚îÇ
    ‚îú‚îÄ Scraping              ‚îú‚îÄ Screenshots
    ‚îú‚îÄ Parsing               ‚îú‚îÄ Performance Analysis
    ‚îî‚îÄ Metadata              ‚îî‚îÄ Image Processing

El cliente solo ve el Servidor A (transparencia total)


================================================================================
TIPS Y MEJORES PR√ÅCTICAS
================================================================================

1. Siempre iniciar servidor de procesamiento PRIMERO

2. Usar workers seg√∫n CPU:
   - 4 workers = uso general
   - 8+ workers = alta concurrencia
   - CPU cores = procesamiento intensivo

3. Para desarrollo: usar 127.0.0.1
   Para producci√≥n: usar 0.0.0.0

4. IPv6 es m√°s r√°pido en redes modernas

5. Screenshots consumen recursos, usar solo cuando necesario

6. Los thumbnails se limitan a 5 por p√°gina (configurable)

7. Timeout de 30s por p√°gina es suficiente para la mayor√≠a

8. Usar batch_scraper.py para m√∫ltiples URLs

================================================================================