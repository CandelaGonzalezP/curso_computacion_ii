# ============================================
# README.md
# ============================================

# TP2 - Sistema de Scraping y AnÃ¡lisis Web Distribuido

Sistema distribuido de scraping web implementado con Python utilizando `asyncio` para operaciones I/O asÃ­ncronas y `multiprocessing` para procesamiento paralelo de tareas CPU-bound.

## ğŸ“‹ CaracterÃ­sticas

- **Servidor AsÃ­ncrono (Parte A)**: Maneja mÃºltiples solicitudes concurrentes usando asyncio
- **Servidor de Procesamiento (Parte B)**: Pool de procesos para tareas computacionalmente intensivas
- **ComunicaciÃ³n Transparente**: El cliente solo interactÃºa con el Servidor A
- **Soporte IPv4/IPv6**: Ambos servidores soportan ambos protocolos
- **Protocolo Binario Eficiente**: ComunicaciÃ³n optimizada entre servidores
- **Manejo Robusto de Errores**: Timeouts, reintentos y fallbacks

## ğŸ—ï¸ Arquitectura

```
Cliente HTTP
    â†“
Servidor Asyncio (A) â† â†’ Servidor Multiprocessing (B)
    â”‚                         â”‚
    â”œâ”€ Scraping              â”œâ”€ Screenshots
    â”œâ”€ Parsing               â”œâ”€ Performance Analysis
    â””â”€ Metadata              â””â”€ Image Processing
```

## ğŸ“ Estructura del Proyecto

```
TP2/
â”œâ”€â”€ server_scraping.py          # Servidor asyncio principal
â”œâ”€â”€ server_processing.py        # Servidor multiprocessing
â”œâ”€â”€ client.py                   # Cliente de prueba
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ html_parser.py          # Parser HTML
â”‚   â”œâ”€â”€ metadata_extractor.py  # Extractor de metadatos
â”‚   â””â”€â”€ async_http.py           # Cliente HTTP asÃ­ncrono
â”œâ”€â”€ processor/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ screenshot.py           # Generador de screenshots
â”‚   â”œâ”€â”€ performance.py          # AnÃ¡lisis de rendimiento
â”‚   â””â”€â”€ image_processor.py      # Procesador de imÃ¡genes
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ protocol.py             # Protocolo de comunicaciÃ³n
â”‚   â””â”€â”€ serialization.py        # SerializaciÃ³n de datos
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â””â”€â”€ test_processor.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone <repository-url>
cd TP2
```

### 2. Crear entorno virtual (recomendado)

```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate  # Windows
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Instalar ChromeDriver (para screenshots)

**Linux (Ubuntu/Debian):**
```bash
sudo apt-get update
sudo apt-get install chromium-chromedriver
```

**macOS:**
```bash
brew install chromedriver
```

**Windows:**
Descargar desde: https://chromedriver.chromium.org/

**Alternativa - Playwright (mÃ¡s fÃ¡cil):**
```bash
pip install playwright
playwright install chromium
```

## ğŸ® Uso

### Iniciar el Servidor de Procesamiento (primero)

```bash
# IPv4
python3 server_processing.py -i 127.0.0.1 -p 9000

# IPv6
python3 server_processing.py -i ::1 -p 9000

# Con mÃ¡s procesos
python3 server_processing.py -i 127.0.0.1 -p 9000 -n 8
```

### Iniciar el Servidor de Scraping

```bash
# IPv4
python3 server_scraping.py -i 127.0.0.1 -p 8000

# IPv6
python3 server_scraping.py -i ::1 -p 8000

# Con mÃ¡s workers
python3 server_scraping.py -i 0.0.0.0 -p 8000 -w 8

# Especificar servidor de procesamiento remoto
python3 server_scraping.py -i 0.0.0.0 -p 8000 --processing-host 192.168.1.100 --processing-port 9000
```

### Usar el Cliente de Prueba

```bash
# Scraping bÃ¡sico
python3 client.py --url https://example.com

# Health check
python3 client.py --health

# Usando POST
python3 client.py --url https://python.org --post

# Salida en JSON
python3 client.py --url https://github.com --json

# Servidor remoto
python3 client.py --url https://example.com --host 192.168.1.10 --port 8000
```

### Usando curl

```bash
# GET request
curl "http://localhost:8000/scrape?url=https://example.com" | jq

# POST request
curl -X POST http://localhost:8000/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://python.org"}' | jq

# Health check
curl http://localhost:8000/health | jq
```

## ğŸ“Š Formato de Respuesta

```json
{
  "url": "https://example.com",
  "timestamp": "2024-11-10T15:30:00Z",
  "status": "success",
  "scraping_data": {
    "title": "Example Domain",
    "links": ["https://...", "..."],
    "meta_tags": {
      "description": "...",
      "keywords": "...",
      "og_title": "..."
    },
    "images_count": 15,
    "structure": {
      "h1": 2,
      "h2": 5,
      "h3": 10
    }
  },
  "processing_data": {
    "screenshot": "base64_encoded_image...",
    "performance": {
      "load_time_ms": 1250,
      "total_size_kb": 2048,
      "num_requests": 45,
      "breakdown": {
        "html_size_kb": 50.5,
        "scripts": 10,
        "stylesheets": 5,
        "images": 30
      }
    },
    "thumbnails": ["base64_thumb1", "base64_thumb2"]
  }
}
```

## ğŸ§ª Testing

```bash
# Ejecutar todos los tests
pytest

# Tests especÃ­ficos
pytest tests/test_scraper.py
pytest tests/test_processor.py

# Con coverage
pytest --cov=. --cov-report=html
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### LÃ­mites y Timeouts

En `server_scraping.py`:
- `max_connections`: LÃ­mite de conexiones concurrentes (default: workers)
- `timeout`: Timeout por pÃ¡gina (default: 30s)

En `server_processing.py`:
- `num_processes`: Procesos en el pool (default: CPU count)
- `task_timeout`: Timeout por tarea (default: 45s)

### OptimizaciÃ³n de Rendimiento

**Para alta concurrencia:**
```bash
# MÃ¡s workers asyncio
python3 server_scraping.py -i 0.0.0.0 -p 8000 -w 20

# MÃ¡s procesos
python3 server_processing.py -i 0.0.0.0 -p 9000 -n 16
```

**Para procesamiento pesado:**
```bash
# Menos workers, mÃ¡s procesos
python3 server_scraping.py -w 4
python3 server_processing.py -n 12
```

## ğŸ› Troubleshooting

### Error: "Connection refused" al servidor de procesamiento

**SoluciÃ³n:** Asegurarse de iniciar primero el servidor de procesamiento

```bash
# Terminal 1
python3 server_processing.py -i 127.0.0.1 -p 9000

# Terminal 2 (despuÃ©s)
python3 server_scraping.py -i 127.0.0.1 -p 8000
```

### Error: "ChromeDriver not found"

**SoluciÃ³n:** Instalar ChromeDriver o usar Playwright

```bash
# OpciÃ³n 1: ChromeDriver
sudo apt-get install chromium-chromedriver

# OpciÃ³n 2: Playwright (mÃ¡s fÃ¡cil)
pip install playwright
playwright install chromium
```

### Error: "Address already in use"

**SoluciÃ³n:** El puerto ya estÃ¡ en uso

```bash
# Encontrar proceso usando el puerto
lsof -i :8000  # Linux/Mac
netstat -ano | findstr :8000  # Windows

# Usar otro puerto
python3 server_scraping.py -i 127.0.0.1 -p 8001
```

### Screenshots no funcionan

**SoluciÃ³n:** Verificar instalaciÃ³n de Selenium/ChromeDriver

```bash
# Test manual
python3 -c "from selenium import webdriver; driver = webdriver.Chrome(); driver.quit()"
```

## ğŸ“š DocumentaciÃ³n TÃ©cnica

### Protocolo de ComunicaciÃ³n

El protocolo entre servidores usa un formato binario simple:

```
[4 bytes: tamaÃ±o del mensaje (big-endian)]
[N bytes: mensaje serializado (pickle)]
```

### SerializaciÃ³n

- **Por defecto**: Pickle (Python nativo, rÃ¡pido, soporta objetos complejos)
- **Fallback**: JSON (interoperabilidad, debugging)

### Flujo de Trabajo

1. Cliente envÃ­a request HTTP al Servidor A
2. Servidor A inicia scraping asÃ­ncrono
3. En paralelo, Servidor A solicita procesamiento al Servidor B
4. Servidor B ejecuta tareas en procesos separados
5. Servidor A consolida resultados y responde al cliente

## ğŸ”’ Seguridad

- **Rate Limiting**: Implementar lÃ­mites por IP/dominio
- **Input Validation**: URLs sanitizadas antes de procesar
- **Resource Limits**: LÃ­mites de memoria y tiempo para prevenir DoS
- **Network Isolation**: Servidor B puede estar en red interna

## ğŸ“ˆ MÃ©tricas de Rendimiento

En hardware promedio (4 cores, 8GB RAM):

- **Throughput**: ~50 pÃ¡ginas/minuto
- **Latencia**: 2-10 segundos por pÃ¡gina (dependiendo del sitio)
- **Memoria**: ~100-500 MB por worker

## ğŸ¤ Contribuciones

Ver archivo `CONTRIBUTING.md` para guÃ­as de contribuciÃ³n.

## ğŸ“ Licencia

Este proyecto es parte del curso de ComputaciÃ³n II.

## ğŸ‘¥ Autores

- Estudiante: [Tu Nombre]
- Curso: ComputaciÃ³n II
- Fecha: Noviembre 2024

## ğŸ“ Soporte

Para problemas o preguntas:
- Crear issue en GitHub
- Consultar con docentes del curso
- Revisar documentaciÃ³n oficial de asyncio y multiprocessing